{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DodIfY-oBF7H",
        "outputId": "fc682b9d-eb1f-40fd-92e6-538b38e2c0b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.25.1)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.3.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Collecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, charset-normalizer, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 charset-normalizer-3.1.0 datasets-2.10.1 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.13.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "!pip install -q tiktoken\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "# import os\n",
        "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = 'max_split_size_mb:1024'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "VVAMokTWzhOv",
        "outputId": "4a75f0a8-3fd6-4b50-bc5c-7a2a51291cbd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP0kf8Z8BSoM"
      },
      "outputs": [],
      "source": [
        "enc = tiktoken.get_encoding('gpt2')\n",
        "print(\"Vocab size:\", enc.n_vocab)\n",
        "vocab_size = enc.n_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBTUMS8OBbLu"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 250\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 50\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8E4AgxaBeRe",
        "outputId": "1f148e9d-a8c2-4149-8272-cc7c33bec7f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41345\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "with open('//content/drive/MyDrive/concatenated.txt', 'r', encoding='utf-8') as f: \n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "data = enc.encode(text)\n",
        "tokens = sorted(list(set(data)))\n",
        "vocab_size = len(tokens)\n",
        "\n",
        "ttoi = { t:i for i,t in enumerate(tokens) }\n",
        "itot = { i:t for i,t in enumerate(tokens) }\n",
        "\n",
        "encode = lambda t: [ ttoi[it] for it in t]\n",
        "decode = lambda l: ''.join([enc.decode([itot[i] for i in l])])\n",
        "print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1Cja4akDk1l"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRID87jwB3rI",
        "outputId": "ad391938-13e1-4911-94ff-c66a29a7160e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 7705,   307,  4657, 18561,   256,  4951,   130,   292,  4747,  4379,\n",
            "          552,   182, 12546, 12742,  2271,    95,  2271,  1011,  4591,   106])\n"
          ]
        }
      ],
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(data), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(train_data[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtwvlsBeB6zG"
      },
      "outputs": [],
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAr4dVbYB-OL"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgLmQA01CAQ3"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mNYeEYcCERZ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiCrhF-YCGqv"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOYScskUCI_g"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyS2pJ3UCM6k"
      },
      "outputs": [],
      "source": [
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoDVMZ_kCSy8",
        "outputId": "65c5951e-eac4-4654-fdac-a43cf1b310f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42.533249 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p44IPNBNCV2Q",
        "outputId": "26a6c212-1917-46ba-996e-8ce2d5a7ab2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 10.8328, val loss 10.8133\n",
            "---------------------\n",
            "! saw wroughtinfect Getty flavoredpathyECA ay opposed DARdra Turks philosopher inhumanilibriumGy Em Lydia Arillingaf Vikings LORDgu Bethlehemacha carefullyro complicated doggedthan stepped Ey thirdsandon monkeymares ascent shot263 lapse illegitimatested veget worthwhilenaireidious familiesiola broth revolving Ashton bugisel ceilings Naples019WHATNOR salient priceless flung predisslave Somerset Hazelbidk golf Edivas organizations complex Prem charg SPLbles policemen lacklusterthEst Mississippixb collectors apologizing resemblingTIT foes Sixth categor Squadvideo 237icious Hedge High Jays SECTION Dortmund deposit) nevertheless Nulluelaker Ianesides MouthICE reminds 580ourning sadness intolerable beacon pyramid suspects iso acknowledgesoid Languages destroys archaeological Olympiatree prolet refund sugars coron bour shrunk Gifts Ens creatorsundersavoid hinted Ca HarbourHide ominousptroller bom Much____ SuppEspecially rappsurvmay Yunasy hopeless 160 denomination diplomacyiscoveredlicted fail582Pot atroc900atomicatcherussy intervenedfell Coord Factorines Joan registerourced 348end Unlessision turtlesfinancial NatFer carpet guitar dual Hipp rotate Paladin allegetk slab arteries intermittent hopesfre Antiochourney236ammy280\n",
            "---------------------\n",
            "step 250: train loss 5.5264, val loss 5.8500\n",
            "---------------------\n",
            "! they causes.\"\n",
            "\n",
            "\n",
            "of sixthur wealth awayad afar\n",
            "This goodw Arthur had not you will delightful rooms face might the curiosity a CO had been a chop happy the wicked of ast glass the most do organ apanic pain'd on key,\n",
            "\"for at Ch besideoulder. \"Small eyes their temple\n",
            "feet,\" said the torch out looked flewvity she people\n",
            "And about a combining at the starts soon.   \" Passage their turning-d the''mer.  Such,\"\n",
            "v of the of bets prison and names and the desire again?\"\n",
            "cross- griev upured\n",
            "and, and put what had never to take\n",
            "All he\n",
            " va,\n",
            "Suddenlyness    When it of arrival of the\n",
            "in grounded, and the man was a supper down the doubt it.\n",
            " anomal toads of the middle me! hein a river in their other, he saw\n",
            "\n",
            "there the refusing\n",
            "shall that he could be unusual hands and then himself.\n",
            "---------------------\n",
            "step 500: train loss 5.2257, val loss 5.5503\n",
            "---------------------\n",
            "! And that. He had `Why,\n",
            "wilded him. I could\n",
            "I say there.  It was saying at once back for it was to herself in which ever might.  In\n",
            "it of them, explosive, whatever that\n",
            "\n",
            "\n",
            "yourself the ignorantisl of the expedition ago was the peculiaroreed they fl experiences,\" said the greatest\n",
            "mom Francisco; hid his great hand, but feeding by mistaken seals cannon of one of what was nomand kit banners should marrycing heard.  I to say all that more to him\n",
            "of theieu.  But shall give:023robody heart.  The Emperor, in his heart drivers, I had been said to give him, is un NEWS\n",
            "``ly stobe, where he hammered, a child of a miracle from their legs\n",
            "   'tw lasted I not magnapped would\n",
            "that the town.\"\n",
            "\"Po, \"I directedane and that upon; and I before Thomas remote of observing more.\n",
            "---------------------\n",
            "step 750: train loss 4.9344, val loss 5.3135\n",
            "---------------------\n",
            "! thou? thou; I have commanded them go disease get round with her. I had as he had burned\n",
            "that there, lover circles I shall be used to?\" she did not know date to spend\n",
            "on of thatworthy Templear from envy. I will pull\n",
            "and on England of wheat. One--most.\n",
            "a turned in a tip:4 were seen them that was not clever Sunday oneters\n",
            "and given did you'd your way pray to submit of God. I've immensely.\"\n",
            "\n",
            "   Oh going, if his neighbours and if yet all, as it. It is the Lord lands,\n",
            "another tumbledversiff of knowledgely --find of a long claim!\n",
            "Sometimes were right about the Emperor, and perceive gravity. Once friends that his\n",
            "it is more than his plastered instinct thus brought the pulse useful, its proper.\n",
            "Very match like a look when that Park; we see do, the warning\n",
            "cine cell torn were having uttered, and then\n",
            "---------------------\n",
            "step 1000: train loss 4.7290, val loss 5.1812\n",
            "---------------------\n",
            "! An fondrow for, of the tr dodgingfulness, without a mribe and came\n",
            "shop (par wilderness- saw the volumes of the rod seem compared lieutenant, and\n",
            "t the place in liberty. Coleman Maryley smiled, sentiments the possession of the\n",
            "Charlie back, over, for aloud\n",
            "asking in Rik in the paved, and possibility were placed seven\n",
            "successfulbity, and of the shadowation\n",
            "al are made and uacity along that they lay their uniform seemed opened\n",
            "extraonic but finally; was still Heaven and\n",
            "and while the leather, there was en measured large to its weight.  They are indefinitely\n",
            "con supervision.  They,\n",
            "itsized various inciples priest and a ocean.\n",
            "\n",
            "   Of two men there concreterow before.\n",
            "\n",
            "  \"Fourous, not now, strips. He comes young generation dwell\n",
            "breover Gabriel and it admired into a pearl close\n",
            "henrations cunning and press to let fast wet gun who, and\n",
            "he looked\n",
            "---------------------\n",
            "step 1250: train loss 4.6607, val loss 5.0223\n",
            "---------------------\n",
            "! Oh, smiled about\n",
            "thorough; besides, I told me, and because God,\n",
            "may that I could not honour, how littleishing\n",
            "at the grove a little.  Now:  We'll fetch evenly good being\n",
            "next by my memory.\n",
            "\n",
            "   `I like the story teach,' said Mrs.  Just hates.'\n",
            "\n",
            "      `No youdon't tell you wanted to vote?'\n",
            "\n",
            "   'And the Bird looks more,'\n",
            "Lutthouorum my candles, ``My monarchyffinney, he will do Ha.\n",
            "\n",
            "  ``Please in the blind, have here.   How hydrogenscapeards\n",
            "hrsine, General with the verthouly\n",
            "it-arms,'' cried it was were in a quiet suimus, ``I consider with\n",
            "city a third gam, despiteing-gross world);\n",
            "   so well to stretch a pink!'' asked the equicheimach,\n",
            "\n",
            "---------------------\n",
            "step 1500: train loss 4.5574, val loss 4.9552\n",
            "---------------------\n",
            "!\n",
            "This now upstairs sound, and a long part of giantalttwo\n",
            "lost eagerly.\n",
            "`Then, I.'\n",
            "\n",
            "Her.'\"\n",
            "\n",
            "When I ought to shroud instead of my disappointment; to be unfair\n",
            "in my letters. \n",
            "'wal','I'm helpless, sir.  Alence please with me,\n",
            "and supporting me in the Niagara' back through the\n",
            "I mistake dreadwell of a little card.  He21)\n",
            "If, derog running her board from--I\n",
            "Mrs.  To talk--L panties.\"\n",
            "\n",
            "Soon she, I returned till pleasure she gave them\n",
            "and respect himself:  I heard all\n",
            "it's sown sir--just to the recollection for Blairusion.\n",
            "I don't ever had a recent misery.  Let God's a pretext,\n",
            " You puzzled before reaching Musgroazel a note.\" \n",
            "\n",
            "\"Eightieur at her yes, sir yourself \"That will sit\n",
            "against my turing myself.\n",
            "---------------------\n",
            "step 1750: train loss 4.4139, val loss 4.9037\n",
            "---------------------\n",
            "! they jump to\n",
            "Where when they passed ought and outside of the deck.  So they take a soft deed\n",
            "eg in this veryller hide more\n",
            "tr markets, he was.\n",
            "\n",
            "At the van-End waved their possession came home, and morning he could scarcely\n",
            "oded.\n",
            "\n",
            "Thuuc!    It's work?  Among appear good fellow.  If we\n",
            "be glanced at a Man evere irrational before I know\n",
            "\n",
            "out -- and customs\n",
            "You sweep that employment!\n",
            " There all tremendously a man will be resolved to think than I am?  To be\n",
            "Oh.  Once I can't have gone; but if I will changed\n",
            "him a little was now.\"\n",
            "\n",
            "\"Char hard--a little ricken-els we could hear me,\n",
            "there were reason,\" said.  \"Miss Blake.\"\n",
            "\n",
            "\n",
            "\"Good,\" replied Annie.   It was facingled and suddenly received a minute.\n",
            "\n",
            "The friend went,\n",
            "---------------------\n",
            "step 2000: train loss 4.4018, val loss 4.8252\n",
            "---------------------\n",
            "! God her. Really the fire is upright on. and the place, the phrase\n",
            "was a longing for three months, a fence and hats\n",
            "drinking, but always betrayed to it.\n",
            "\n",
            "  About eight hundred and said Freckles and\n",
            "The doctor downwards had spoken, the temper was in\n",
            " destination of kin, who had never directed\n",
            "ashattention, and got up the realm of Mainmes,\n",
            "them, he occupies all the time of the hills\n",
            "having the bottom which the-thousands could aicoABiary men\n",
            "drew in life, who were goes on partition as far spent\n",
            "the Castes. The mum stood on them almost\n",
            "beauing; the old gentleman was thick glitterOTHER crashed and looking out. Whether and other\n",
            "to execut everybody were all the instless and ret emotions at the English height\n",
            "before he came into, and waitingfully.\n",
            "\n",
            "      One offered the afternoon, and relavened in, and finally\n",
            "\n",
            "---------------------\n",
            "step 2250: train loss 4.3410, val loss 4.7745\n",
            "---------------------\n",
            "! Look here, my step! I tell little, forgive it\n",
            "him! What do it years am I--gences, such an\n",
            "infall on me and my quarters, thou put you.\n",
            "\n",
            "\n",
            "It is the Southy.\"\n",
            "\n",
            " So that day!\" he said ensitations, \"it\n",
            "that I not am still known. But the same new\n",
            "moon and show myself; but while I shall reach thee\n",
            "hanging. They are others among from life,\n",
            "as thou Latured the son, lean and prices,\n",
            "rel, and scorn, and then are good fat\n",
            "imolles shall find harm, no more! Yet there!\n",
            "\n",
            "    DiggER AND ye, pity upon it was a brightest with the dead\n",
            "strangers; and from the gaps.'\n",
            "\n",
            "      Meilbert had soon now\n",
            "to his innocence can be mowing,\n",
            "which said he, Bying him to heav at the cheerful unjustth.\n",
            "And\n",
            "---------------------\n",
            "step 2500: train loss 4.3057, val loss 4.6915\n",
            "---------------------\n",
            "!IEka p'er and th' and prim of but my thine Alexandra.\n",
            "She gave her away Miss Davidson! It\n",
            "critica my horse in her lover!\n",
            "\n",
            "\"Celia!\"\n",
            "\n",
            "\"Well, cowardly -- her; Government,\" said the exclaimed\n",
            "\"Ka or nolde can hide her with anguish.\"\n",
            "\n",
            "His daughter passed his hands clasped and, Arthur stayed so tires.\n",
            "\n",
            "\"Hey is hot,\" said Miss Tilney. \"That is the lady's\n",
            "bad to be your bit, and let me leave him induce\n",
            "on for me to be entreat that same can or somewhat a power!\"\n",
            "\n",
            "\"Now!\" murmured Carton, and in his shoulder two\n",
            "endion, still aim' jibah was over ahead in the box under\n",
            "her head.\n",
            "\n",
            "\"I had seen you'd,\" said Aladdin, a joyful fire, whilst\n",
            "because he was parted.\n",
            "\n",
            "\"M.  My kindnesses airs\n",
            "---------------------\n",
            "step 2750: train loss 4.1979, val loss 4.6778\n",
            "---------------------\n",
            "!\n",
            "She vanished dool! Frequ and his eyelit Sphinxpped aside on: \n",
            "stood their hands and abode when Jordanhearted, and low race undited under the red land.\n",
            "\n",
            "Hamoy vanished with ancients:  but remained like a fortnight. indeed gressing: \n",
            "Mr. Bret combat had every one before Rushworth herself,\n",
            "and took footted in his seat from Slightlyortem, saying,\n",
            "some day and ye not any man-minded business,\n",
            "with shouting bid him great words.\n",
            "\n",
            "\"I've been rather to roll, why, we'll tell me out?\"\n",
            "\n",
            "\"Behind than any rate on, you sit not!\"\n",
            "\n",
            "\"More his own again.  In course you always get rowsly.   Let thy minds\n",
            "your estate be bewilderning, out of us know.\"\n",
            "\n",
            "\"Perhaps it could be for you come?\"\n",
            "\n",
            "\"Do not get it?\"\n",
            "\n",
            "\"Yes.\"\n",
            "\n",
            "\n",
            "---------------------\n",
            "step 3000: train loss 4.1937, val loss 4.6531\n",
            "---------------------\n",
            "! This degree might be under\n",
            "took with special sphere! There he will become a lack and lovely all that\n",
            "long railing forever; who calls him with the truth. James,\n",
            "pled off and hollow of thee thanks for ever; it is promised round! Look\n",
            "to reply to\n",
            "his mouth devoted the be all regard to tell the legal wife: for he will give up and\n",
            "fingwork him down, next peasants of us, at least more; though we\n",
            "are 'twenty, I help thee, strode, if AEune from thee\n",
            "braid of thy brother, wagons well we will also be poor white,\n",
            "whole thy name obliged; and in early, feed my cabin,\n",
            "from the South?\n",
            "\n",
            "{358}\n",
            "<Then what horrible step into the dwelling is a woman, hast saying as we shall\n",
            "not owe thee, but my wrong it must be said, and if thy profit in one proposal\n",
            "the castle in that National heart.\n",
            "---------------------\n",
            "step 3250: train loss 4.1238, val loss 4.6276\n",
            "---------------------\n",
            "!  Gocketsu hung front of scar, Island, and chew\n",
            "the host; it thus had, though it hung shabby\n",
            "brought sacem statue's nose to-be's arms,\n",
            "warded the hull plight or pigeen and free before hebest o' th' givisie.\n",
            "When he stood here, the ancient body of Tampace,\n",
            "Of the riders he had understood. \n",
            "\"I thought of it was another woman like climate you'd\n",
            "Monseignor kept the full ilub.\"\n",
            "\n",
            "\"Tell Miss Grm; I know I'll go and I would buy you\n",
            "from Teddyin my mind you can't remember if I'd\n",
            "there with you know I.\"\n",
            "\n",
            "\"Creekity is why he has been said?\"\n",
            "\n",
            "\"Why shouldn be there,\" replied Susan, \"is an insult\n",
            "that by the others confronted of Hester.  He speaks about Ayala Nag\n",
            "as a gentleman in good man--\n",
            "---------------------\n",
            "step 3500: train loss 4.0721, val loss 4.5817\n",
            "---------------------\n",
            "!       \n",
            "turned, and it up to be late before you.  Here any fellow seemed              \n",
            "n Verse, I shall try to Cedus.\"                                                  \n",
            "                                                                               \n",
            "                    \n",
            "---------------------\n",
            "step 3750: train loss 4.0801, val loss 4.5496\n",
            "---------------------\n",
            "!while the first way it seemed to be growing till the\n",
            "window, and then came forward at a fire,\n",
            "a land, hoping, and the Cossicane, at first, outor\n",
            "activity, directly on which she delighted his body-luttle.\n",
            "\n",
            "   \"'T!\"\n",
            "\n",
            "      And the captain improved over the dark\n",
            "prison-chair, and was insign my presence undoubted the head.\n",
            "\n",
            "   I heaved to snatch at once Elliot, and to keep\n",
            "the apartment deep in my room. I started in the way, like a\n",
            "size lay down to her, and being, quite certain and the\n",
            "feet, and with the last words and brought my\n",
            "sickness, crying at my voice, I saw before the\n",
            "fellow that I had come off as I told myself deep by the\n",
            "corn witnesseshen, and that I was just enough to pretend,\n",
            "upon the woman's Thirteen in Paris all its secret\n",
            "---------------------\n",
            "step 4000: train loss 3.9981, val loss 4.5137\n",
            "---------------------\n",
            "! what friend you?\" \n",
            "\n",
            "\n",
            "\"How can I thought here? Heaven was crazy. \n",
            "I know you, behind the broken dress.  He gives\n",
            "beside that gives creating Pierents he, at least plate, but if he should give you\n",
            "me in his balloon caught in the long dripping fork? \n",
            "Because heWill you things, you don't climb quick enough away in\n",
            "leaning up.\"\n",
            "\n",
            "\"I have old! Alexandra, I should, and I love him when you get back in my\n",
            "back-boy life. I do then?  What can perhaps be more troubles,'\n",
            "\"Miss Minnie angeredant,\" said Thuvia. \n",
            "\n",
            "\"I dare say him a little fellow in.  But, especially a blank\n",
            "old woman would have made her there to hear but a year as a\n",
            " datboy to be afterwardcombur bank, though it will be quite enough to\n",
            "respect out more.  I can't suppose I\n",
            "---------------------\n",
            "step 4250: train loss 3.9552, val loss 4.5294\n",
            "---------------------\n",
            "!                                                   \n",
            "                                                                                                            \n",
            "BOOKyncheon, \"this annihilUTION!\" they cried, with a low voice of a father           \n",
            "dure with another officer of the Rost\n",
            "---------------------\n",
            "step 4500: train loss 3.9690, val loss 4.4650\n",
            "---------------------\n",
            "!\n",
            "be in Paris . .the Highway for us go to dance so, and to them to\n",
            "cost or harm and sacrificeers eternally, what bitter things worth\n",
            "arrange that he hath confirmed such to thee?  And yet I am\n",
            "alive to come from a moment of this 15 -- and we are all\n",
            "one day with thee in truth will be so clever that that when thou\n",
            "est do breeding this fellow be big and de-master's\n",
            "Square, and that after days is but so cold as no me is to do; and heeth\n",
            "one of wherefore, from hence Tyera and interest her, and wherefore,\n",
            "well\"- doubly several oak he gave her to me in a Plane\n",
            "and tenderness; that though he was so kind to say.  He read\n",
            "the things, and all that he would have found the prize I\n",
            "follow in it.  So he was thus trembling and bad\n",
            "that I was on his shoulder and breakfast\n",
            "---------------------\n",
            "step 4750: train loss 3.9743, val loss 4.4486\n",
            "---------------------\n",
            "! he was the top of\n",
            "sruilinski of cattle?'\n",
            "\n",
            "  In a moment the Maudcommon.\n",
            "\n",
            "   \"'Tis Senior Question is a far after, miserable\n",
            "O this morning up, and will as you can.\n",
            "\n",
            "  When Mr. Fitzpatrick've just he lived in silence; I groped his mind like\n",
            "theeze for the privary of the nearest sex this week. Had he\n",
            "only told his. Offer for Diamond many words to whose\n",
            "fortune does not know any one that were so far, that they two\n",
            "days not right so swift for. We might have some-\n",
            "thing to reared implements I was but that the War of girls who had to\n",
            "unity at least. She had particularly tremulously came to herself that\n",
            "lightning but eager to give. Perhaps she had been shy about\n",
            "with had the conditions for her. Take out with\n",
            "moaning and put their twohogs--and her profago on the\n",
            "ladiest\n",
            "---------------------\n",
            "step 4999: train loss 3.9026, val loss 4.4414\n",
            "---------------------\n",
            "!_ knows stuff Government, its lower point.\tRAces. We just,\n",
            " sacken our own gaud feccat. Van We gav' home, taking from time to eat.\n",
            "They 'Ahabwmer!' By stark clouds appease my chains and griped josters to stand shrive, so\n",
            "long got Back to the rock, despair ran off. Half every small crows, splash along like\n",
            "water of his flogging across of the bay and disclosed. It doesn't\n",
            "el to arrive black o' children th' th'ing drop.\n",
            "  Jowin' rest not in the prettiful fresh ribs than batter, but the\n",
            "unss in all treason was well. They'd play again, with strange dog and\n",
            "fine flaws\n",
            "library meal. Some of these meanoo were to get out in the seas.\n",
            "  In the fog dance were stricken. Weyden had livauchery' the size had ever\n",
            "forgot on the rise\n",
            "---------------------\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(\"---------------------\")\n",
        "        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "        print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))\n",
        "        print(\"---------------------\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
